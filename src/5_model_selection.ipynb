{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b0eed87",
   "metadata": {},
   "source": [
    "# Model Selection for SLP Prediction\n",
    "\n",
    "This notebook performs model selection to predict the `slp` column using various machine learning algorithms with time series cross-validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b7581f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05222b9e",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "253b1834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (3560, 9)\n",
      "\n",
      "Columns: ['idx', 'date', 'day_of_year', 'slp', 'apparent_temperature_mean', 'temperature_2m_mean', 'apparent_temperature_max', 'temperature_2m_max', 'sunrise']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>date</th>\n",
       "      <th>day_of_year</th>\n",
       "      <th>slp</th>\n",
       "      <th>apparent_temperature_mean</th>\n",
       "      <th>temperature_2m_mean</th>\n",
       "      <th>apparent_temperature_max</th>\n",
       "      <th>temperature_2m_max</th>\n",
       "      <th>sunrise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2016-01-01 00:00:00+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1935724.47983604</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.2</td>\n",
       "      <td>4.3</td>\n",
       "      <td>26400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-01-02 00:00:00+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2410157.74830502</td>\n",
       "      <td>-6.9</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>26340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2016-01-03 00:00:00+00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>3009969.53122598</td>\n",
       "      <td>-13.2</td>\n",
       "      <td>-6.8</td>\n",
       "      <td>-11.8</td>\n",
       "      <td>-5.4</td>\n",
       "      <td>26340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2016-01-04 00:00:00+00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>3201452.44485292</td>\n",
       "      <td>-11.5</td>\n",
       "      <td>-5.7</td>\n",
       "      <td>-9.1</td>\n",
       "      <td>-3.5</td>\n",
       "      <td>26340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2016-01-05 00:00:00+00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>3169937.6386627</td>\n",
       "      <td>-11.3</td>\n",
       "      <td>-5.8</td>\n",
       "      <td>-10.6</td>\n",
       "      <td>-5.2</td>\n",
       "      <td>26340</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx                       date  day_of_year               slp  \\\n",
       "0    0  2016-01-01 00:00:00+00:00            1  1935724.47983604   \n",
       "1    1  2016-01-02 00:00:00+00:00            2  2410157.74830502   \n",
       "2    2  2016-01-03 00:00:00+00:00            3  3009969.53122598   \n",
       "3    3  2016-01-04 00:00:00+00:00            4  3201452.44485292   \n",
       "4    4  2016-01-05 00:00:00+00:00            5   3169937.6386627   \n",
       "\n",
       "  apparent_temperature_mean temperature_2m_mean apparent_temperature_max  \\\n",
       "0                      -0.4                 2.4                      2.2   \n",
       "1                      -6.9                -1.5                     -0.7   \n",
       "2                     -13.2                -6.8                    -11.8   \n",
       "3                     -11.5                -5.7                     -9.1   \n",
       "4                     -11.3                -5.8                    -10.6   \n",
       "\n",
       "  temperature_2m_max  sunrise  \n",
       "0                4.3    26400  \n",
       "1                2.5    26340  \n",
       "2               -5.4    26340  \n",
       "3               -3.5    26340  \n",
       "4               -5.2    26340  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = 'dataset/df_clean_rf.csv'\n",
    "df = pd.read_csv(dataset, sep=';', decimal=',')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d190b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns 'rlm' and 'entry' not found, dataset unchanged.\n",
      "Dataset shape after removing rlm and entry (if present): (3560, 9)\n",
      "\n",
      "Remaining columns: ['idx', 'date', 'day_of_year', 'slp', 'apparent_temperature_mean', 'temperature_2m_mean', 'apparent_temperature_max', 'temperature_2m_max', 'sunrise']\n"
     ]
    }
   ],
   "source": [
    "# Remove 'rlm' and/or 'entry' columns if present\n",
    "to_remove = [col for col in ['rlm', 'entry'] if col in df.columns]\n",
    "if to_remove:\n",
    "    df_clean = df.drop(columns=to_remove)\n",
    "    print(f\"Removed columns: {to_remove}\")\n",
    "else:\n",
    "    df_clean = df.copy()\n",
    "    print(\"Columns 'rlm' and 'entry' not found, dataset unchanged.\")\n",
    "\n",
    "print(f\"Dataset shape after removing rlm and entry (if present): {df_clean.shape}\")\n",
    "print(f\"\\nRemaining columns: {df_clean.columns.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b918bbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse date and sort by date (important for time series)\n",
    "df_clean['date'] = pd.to_datetime(df_clean['date'])\n",
    "df_clean = df_clean.sort_values('date').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "770274c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (3560, 7)\n",
      "Target shape: (3560,)\n",
      "\n",
      "Boolean columns (not scaled): ['holiday']\n",
      "Categorical columns (not scaled): ['weathercode']\n",
      "Cyclical columns (will be sin/cos encoded): ['day_of_week', 'day_of_year', 'winddirection_10m_dominant']\n",
      "Continuous columns (will be scaled): ['idx', 'apparent_temperature_mean', 'temperature_2m_mean', 'apparent_temperature_max', 'temperature_2m_max', 'sunrise']\n"
     ]
    }
   ],
   "source": [
    "# Separate features and target\n",
    "if dataset == 'dataset/df_clean_stat.csv':\n",
    "    X = df_clean.drop(columns=['date', \"Residential (SLP)\"])\n",
    "    y = df_clean['Residential (SLP)']\n",
    "else:\n",
    "    X = df_clean.drop(columns=['date', 'slp'])\n",
    "    y = df_clean['slp']\n",
    "\n",
    "# Define feature types for proper preprocessing\n",
    "boolean_cols = ['holiday']\n",
    "categorical_cols = ['weathercode']  # Leave as-is for tree-based models\n",
    "cyclical_cols = ['day_of_week', 'day_of_year', 'winddirection_10m_dominant']\n",
    "\n",
    "# All other columns are continuous and should be scaled\n",
    "continuous_cols = [col for col in X.columns \n",
    "                   if col not in boolean_cols + categorical_cols + cyclical_cols]\n",
    "\n",
    "# Convert continuous columns to numeric (CSV has mixed decimal formats: some use ',' some use '.')\n",
    "for col in continuous_cols:\n",
    "    X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "\n",
    "y = pd.to_numeric(y, errors='coerce')\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nBoolean columns (not scaled): {boolean_cols}\")\n",
    "print(f\"Categorical columns (not scaled): {categorical_cols}\")\n",
    "print(f\"Cyclical columns (will be sin/cos encoded): {cyclical_cols}\")\n",
    "print(f\"Continuous columns (will be scaled): {continuous_cols}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253e5877",
   "metadata": {},
   "source": [
    "## 2. Feature Preprocessing\n",
    "\n",
    "- **Boolean features** (holiday): Left unchanged (0/1)\n",
    "- **Categorical features** (weathercode): Left unchanged (tree-based models handle them well)\n",
    "- **Cyclical features** (day_of_week, day_of_year, winddirection): Sin/cos encoding to preserve circular nature\n",
    "- **Continuous features**: StandardScaler normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0786dd0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after cyclical encoding: (3560, 7)\n",
      "\n",
      "New cyclical features added:\n",
      "  day_of_week -> day_of_week_sin, day_of_week_cos\n",
      "  day_of_year -> day_of_year_sin, day_of_year_cos\n",
      "  winddirection_10m_dominant -> winddirection_10m_dominant_sin, winddirection_10m_dominant_cos\n",
      "\n",
      "All features: ['idx', 'day_of_year', 'apparent_temperature_mean', 'temperature_2m_mean', 'apparent_temperature_max', 'temperature_2m_max', 'sunrise']\n"
     ]
    }
   ],
   "source": [
    "# Apply cyclical encoding for cyclical features\n",
    "# This preserves the circular nature of these variables (e.g., day 365 is close to day 1)\n",
    "\n",
    "def cyclical_encode(df, col, max_val):\n",
    "    \"\"\"Encode a cyclical feature using sine and cosine transformation.\"\"\"\n",
    "    df[f'{col}_sin'] = np.sin(2 * np.pi * df[col] / max_val)\n",
    "    df[f'{col}_cos'] = np.cos(2 * np.pi * df[col] / max_val)\n",
    "    return df\n",
    "\n",
    "# Create a copy and apply cyclical encoding\n",
    "X_encoded = X.copy()\n",
    "\n",
    "if dataset == 'dataset/data_v2_full.csv':\n",
    "    # Encode day_of_week (0-6, period=7)\n",
    "    X_encoded = cyclical_encode(X_encoded, 'day_of_week', 7)\n",
    "    \n",
    "    # Encode wind direction (0-360 degrees, period=360)\n",
    "    X_encoded = cyclical_encode(X_encoded, 'winddirection_10m_dominant', 360)\n",
    "\n",
    "    # Encode day_of_year (1-366, period=366)'day_of_week', 'd\n",
    "    X_encoded = cyclical_encode(X_encoded, 'day_of_year', 366)    \n",
    "\n",
    "    # Drop original cyclical columns (replaced by sin/cos versions)    \n",
    "    X_encoded = X_encoded.drop(columns=cyclical_cols)\n",
    "\n",
    "print(f\"Shape after cyclical encoding: {X_encoded.shape}\")\n",
    "print(f\"\\nNew cyclical features added:\")\n",
    "for col in cyclical_cols:\n",
    "    print(f\"  {col} -> {col}_sin, {col}_cos\")\n",
    "print(f\"\\nAll features: {X_encoded.columns.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c361f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuous feature statistics before scaling:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>idx</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3559.0</td>\n",
       "      <td>1779.500000</td>\n",
       "      <td>1027.827807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apparent_temperature_mean</th>\n",
       "      <td>-14.7</td>\n",
       "      <td>29.6</td>\n",
       "      <td>8.742556</td>\n",
       "      <td>9.013316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>temperature_2m_mean</th>\n",
       "      <td>-10.2</td>\n",
       "      <td>29.1</td>\n",
       "      <td>11.075225</td>\n",
       "      <td>7.463579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apparent_temperature_max</th>\n",
       "      <td>-13.0</td>\n",
       "      <td>36.7</td>\n",
       "      <td>12.627893</td>\n",
       "      <td>9.957706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>temperature_2m_max</th>\n",
       "      <td>-6.7</td>\n",
       "      <td>36.1</td>\n",
       "      <td>14.928006</td>\n",
       "      <td>8.401043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sunrise</th>\n",
       "      <td>9960.0</td>\n",
       "      <td>26400.0</td>\n",
       "      <td>17927.696629</td>\n",
       "      <td>5540.841244</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              min      max          mean          std\n",
       "idx                           0.0   3559.0   1779.500000  1027.827807\n",
       "apparent_temperature_mean   -14.7     29.6      8.742556     9.013316\n",
       "temperature_2m_mean         -10.2     29.1     11.075225     7.463579\n",
       "apparent_temperature_max    -13.0     36.7     12.627893     9.957706\n",
       "temperature_2m_max           -6.7     36.1     14.928006     8.401043\n",
       "sunrise                    9960.0  26400.0  17927.696629  5540.841244"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check feature ranges before scaling (continuous features only)\n",
    "print(\"Continuous feature statistics before scaling:\")\n",
    "X_encoded[continuous_cols].describe().T[['min', 'max', 'mean', 'std']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdacc195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature preprocessing summary:\n",
      "  - Boolean features (unchanged): ['holiday']\n",
      "  - Categorical features (unchanged): ['weathercode']\n",
      "  - Cyclical features (sin/cos encoded): ['day_of_week_sin, day_of_week_cos', 'day_of_year_sin, day_of_year_cos', 'winddirection_10m_dominant_sin, winddirection_10m_dominant_cos']\n",
      "  - Continuous features (standardized): 6 columns\n",
      "\n",
      "Final feature matrix shape: (3560, 7)\n",
      "\n",
      "Continuous feature statistics after scaling:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>idx</th>\n",
       "      <td>-1.731564</td>\n",
       "      <td>1.731564</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.00014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apparent_temperature_mean</th>\n",
       "      <td>-2.601246</td>\n",
       "      <td>2.314395</td>\n",
       "      <td>-3.193450e-17</td>\n",
       "      <td>1.00014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>temperature_2m_mean</th>\n",
       "      <td>-2.850940</td>\n",
       "      <td>2.415370</td>\n",
       "      <td>-3.193450e-17</td>\n",
       "      <td>1.00014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apparent_temperature_max</th>\n",
       "      <td>-2.574036</td>\n",
       "      <td>2.417775</td>\n",
       "      <td>-6.386901e-17</td>\n",
       "      <td>1.00014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>temperature_2m_max</th>\n",
       "      <td>-2.574805</td>\n",
       "      <td>2.520517</td>\n",
       "      <td>6.386901e-17</td>\n",
       "      <td>1.00014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sunrise</th>\n",
       "      <td>-1.438196</td>\n",
       "      <td>1.529279</td>\n",
       "      <td>1.916070e-16</td>\n",
       "      <td>1.00014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                min       max          mean      std\n",
       "idx                       -1.731564  1.731564  0.000000e+00  1.00014\n",
       "apparent_temperature_mean -2.601246  2.314395 -3.193450e-17  1.00014\n",
       "temperature_2m_mean       -2.850940  2.415370 -3.193450e-17  1.00014\n",
       "apparent_temperature_max  -2.574036  2.417775 -6.386901e-17  1.00014\n",
       "temperature_2m_max        -2.574805  2.520517  6.386901e-17  1.00014\n",
       "sunrise                   -1.438196  1.529279  1.916070e-16  1.00014"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply StandardScaler ONLY to continuous features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Start with the encoded data\n",
    "X_scaled = X_encoded.copy()\n",
    "\n",
    "# Scale only continuous columns\n",
    "X_scaled[continuous_cols] = scaler.fit_transform(X_encoded[continuous_cols])\n",
    "\n",
    "print(\"Feature preprocessing summary:\")\n",
    "print(f\"  - Boolean features (unchanged): {boolean_cols}\")\n",
    "print(f\"  - Categorical features (unchanged): {categorical_cols}\")\n",
    "print(f\"  - Cyclical features (sin/cos encoded): {[f'{c}_sin, {c}_cos' for c in cyclical_cols]}\")\n",
    "print(f\"  - Continuous features (standardized): {len(continuous_cols)} columns\")\n",
    "print(f\"\\nFinal feature matrix shape: {X_scaled.shape}\")\n",
    "print(f\"\\nContinuous feature statistics after scaling:\")\n",
    "X_scaled[continuous_cols].describe().T[['min', 'max', 'mean', 'std']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e05e439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export preprocessed data to CSV\n",
    "export_df = X_scaled.copy()\n",
    "export_df['slp'] = y.values\n",
    "export_df['date'] = df_clean['date'].values\n",
    "\n",
    "# Reorder columns to put date first\n",
    "cols = ['date', 'slp'] + [col for col in export_df.columns if col not in ['date', 'slp']]\n",
    "export_df = export_df[cols]\n",
    "\n",
    "# Save to dataset folder\n",
    "# export_df.to_csv('dataset/data_v2_step_5.csv', sep=';', decimal=',', index=False)\n",
    "# print(f\"Exported preprocessed data to 'dataset/data_v2_step_5.csv'\")\n",
    "# print(f\"Shape: {export_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9063a7cc",
   "metadata": {},
   "source": [
    "## 3. Time Series Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91c74604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Series Cross-Validation Splits:\n",
      "==================================================\n",
      "Fold 1:\n",
      "  Train: 2016-01-01 to 2016-11-25 (330 samples)\n",
      "  Test:  2016-11-26 to 2017-10-14 (323 samples)\n",
      "\n",
      "Fold 2:\n",
      "  Train: 2016-01-01 to 2017-10-14 (653 samples)\n",
      "  Test:  2017-10-15 to 2018-09-02 (323 samples)\n",
      "\n",
      "Fold 3:\n",
      "  Train: 2016-01-01 to 2018-09-02 (976 samples)\n",
      "  Test:  2018-09-03 to 2019-07-22 (323 samples)\n",
      "\n",
      "Fold 4:\n",
      "  Train: 2016-01-01 to 2019-07-22 (1299 samples)\n",
      "  Test:  2019-07-23 to 2020-06-09 (323 samples)\n",
      "\n",
      "Fold 5:\n",
      "  Train: 2016-01-01 to 2020-06-09 (1622 samples)\n",
      "  Test:  2020-06-10 to 2021-04-28 (323 samples)\n",
      "\n",
      "Fold 6:\n",
      "  Train: 2016-01-01 to 2021-04-28 (1945 samples)\n",
      "  Test:  2021-04-29 to 2022-03-17 (323 samples)\n",
      "\n",
      "Fold 7:\n",
      "  Train: 2016-01-01 to 2022-03-17 (2268 samples)\n",
      "  Test:  2022-03-18 to 2023-02-03 (323 samples)\n",
      "\n",
      "Fold 8:\n",
      "  Train: 2016-01-01 to 2023-02-03 (2591 samples)\n",
      "  Test:  2023-02-04 to 2023-12-23 (323 samples)\n",
      "\n",
      "Fold 9:\n",
      "  Train: 2016-01-01 to 2023-12-23 (2914 samples)\n",
      "  Test:  2023-12-24 to 2024-11-10 (323 samples)\n",
      "\n",
      "Fold 10:\n",
      "  Train: 2016-01-01 to 2024-11-10 (3237 samples)\n",
      "  Test:  2024-11-11 to 2025-09-30 (323 samples)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use TimeSeriesSplit for proper time series cross-validation\n",
    "# This ensures we always train on past data and test on future data\n",
    "tscv = TimeSeriesSplit(n_splits=10)\n",
    "\n",
    "# Visualize the splits\n",
    "print(\"Time Series Cross-Validation Splits:\")\n",
    "print(\"=\"*50)\n",
    "for fold, (train_idx, test_idx) in enumerate(tscv.split(X_scaled)):\n",
    "    train_dates = df_clean.iloc[train_idx]['date']\n",
    "    test_dates = df_clean.iloc[test_idx]['date']\n",
    "    print(f\"Fold {fold + 1}:\")\n",
    "    print(f\"  Train: {train_dates.min().date()} to {train_dates.max().date()} ({len(train_idx)} samples)\")\n",
    "    print(f\"  Test:  {test_dates.min().date()} to {test_dates.max().date()} ({len(test_idx)} samples)\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dcc0a3",
   "metadata": {},
   "source": [
    "## 4. Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5949c065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total models to evaluate: 12\n",
      "  - Linear Regression\n",
      "  - Ridge Regression\n",
      "  - Lasso Regression\n",
      "  - ElasticNet\n",
      "  - Decision Tree\n",
      "  - Random Forest\n",
      "  - Gradient Boosting\n",
      "  - AdaBoost\n",
      "  - XGBoost\n",
      "  - LightGBM\n",
      "  - K-Nearest Neighbors\n",
      "  - SVR\n"
     ]
    }
   ],
   "source": [
    "# Define different types of models with standard parameters\n",
    "models = {\n",
    "    # Linear Models\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(),\n",
    "    'Lasso Regression': Lasso(),\n",
    "    'ElasticNet': ElasticNet(),\n",
    "    \n",
    "    # Tree-based Models\n",
    "    'Decision Tree': DecisionTreeRegressor(random_state=42),\n",
    "    'Random Forest': RandomForestRegressor(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(random_state=42),\n",
    "    'AdaBoost': AdaBoostRegressor(random_state=42),\n",
    "    'XGBoost': XGBRegressor(random_state=42, n_jobs=-1, verbosity=0),\n",
    "    'LightGBM': LGBMRegressor(random_state=42, n_jobs=-1, verbose=-1),\n",
    "    \n",
    "    # Distance-based Models\n",
    "    'K-Nearest Neighbors': KNeighborsRegressor(),\n",
    "    \n",
    "    # Support Vector Machine (tuned for large target values)\n",
    "    'SVR': SVR(C=1e6, epsilon=1e4, kernel='rbf'),\n",
    "}\n",
    "\n",
    "print(f\"Total models to evaluate: {len(models)}\")\n",
    "for name in models.keys():\n",
    "    print(f\"  - {name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a329e0",
   "metadata": {},
   "source": [
    "## 5. Train and Test Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28777418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y, tscv):\n",
    "    \"\"\"\n",
    "    Evaluate a model using time series cross-validation.\n",
    "    Returns average metrics across all folds.\n",
    "    \"\"\"\n",
    "    rmse_scores = []\n",
    "    mae_scores = []\n",
    "    r2_scores = []\n",
    "    \n",
    "    for train_idx, test_idx in tscv.split(X):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        rmse_scores.append(rmse)\n",
    "        mae_scores.append(mae)\n",
    "        r2_scores.append(r2)\n",
    "    \n",
    "    return {\n",
    "        'RMSE_mean': np.mean(rmse_scores),\n",
    "        'RMSE_std': np.std(rmse_scores),\n",
    "        'MAE_mean': np.mean(mae_scores),\n",
    "        'MAE_std': np.std(mae_scores),\n",
    "        'R2_mean': np.mean(r2_scores),\n",
    "        'R2_std': np.std(r2_scores),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9816c5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluating models...\n",
      "============================================================\n",
      "Training: Linear Regression... Done! (R² = 0.8961)\n",
      "Training: Ridge Regression... Done! (R² = 0.8965)\n",
      "Training: Lasso Regression... Done! (R² = 0.8851)\n",
      "Training: ElasticNet... Done! (R² = 0.8870)\n",
      "Training: Decision Tree... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! (R² = 0.8715)\n",
      "Training: Random Forest... Done! (R² = 0.9428)\n",
      "Training: Gradient Boosting... Done! (R² = 0.9417)\n",
      "Training: AdaBoost... Done! (R² = 0.8683)\n",
      "Training: XGBoost... Done! (R² = 0.9342)\n",
      "Training: LightGBM... Done! (R² = 0.9422)\n",
      "Training: K-Nearest Neighbors... Done! (R² = 0.8638)\n",
      "Training: SVR... Done! (R² = 0.8163)\n",
      "\n",
      "All models trained!\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate all models\n",
    "results = {}\n",
    "\n",
    "print(\"Training and evaluating models...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training: {name}...\", end=\" \")\n",
    "    try:\n",
    "        metrics = evaluate_model(model, X_scaled, y, tscv)\n",
    "        results[name] = metrics\n",
    "        print(f\"Done! (R² = {metrics['R2_mean']:.4f})\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        results[name] = {'RMSE_mean': np.nan, 'MAE_mean': np.nan, 'R2_mean': np.nan}\n",
    "\n",
    "print(\"\\nAll models trained!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c5296f",
   "metadata": {},
   "source": [
    "## 6. Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aea8d55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODEL COMPARISON RESULTS (sorted by R² score)\n",
      "================================================================================\n",
      "\n",
      "Metrics averaged over 10-fold Time Series Cross-Validation:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>R²</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>156257.64 ± 49933.23</td>\n",
       "      <td>97521.36 ± 20437.54</td>\n",
       "      <td>0.9428 ± 0.0481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LightGBM</th>\n",
       "      <td>160996.91 ± 47955.83</td>\n",
       "      <td>104886.76 ± 22178.11</td>\n",
       "      <td>0.9422 ± 0.0450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boosting</th>\n",
       "      <td>161645.37 ± 51150.56</td>\n",
       "      <td>105308.97 ± 24420.13</td>\n",
       "      <td>0.9417 ± 0.0469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>176521.35 ± 43506.95</td>\n",
       "      <td>117056.09 ± 31471.16</td>\n",
       "      <td>0.9342 ± 0.0426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge Regression</th>\n",
       "      <td>219276.49 ± 48004.33</td>\n",
       "      <td>164327.65 ± 28665.78</td>\n",
       "      <td>0.8965 ± 0.0654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear Regression</th>\n",
       "      <td>219938.01 ± 47881.02</td>\n",
       "      <td>164794.68 ± 28645.08</td>\n",
       "      <td>0.8961 ± 0.0652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ElasticNet</th>\n",
       "      <td>233678.26 ± 37408.87</td>\n",
       "      <td>180186.17 ± 22737.91</td>\n",
       "      <td>0.8870 ± 0.0577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso Regression</th>\n",
       "      <td>235249.19 ± 53711.19</td>\n",
       "      <td>180281.78 ± 45820.19</td>\n",
       "      <td>0.8851 ± 0.0631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>227238.24 ± 95180.41</td>\n",
       "      <td>139010.16 ± 53233.58</td>\n",
       "      <td>0.8715 ± 0.1326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoost</th>\n",
       "      <td>238869.93 ± 86784.19</td>\n",
       "      <td>192173.84 ± 78219.22</td>\n",
       "      <td>0.8683 ± 0.0875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K-Nearest Neighbors</th>\n",
       "      <td>259447.02 ± 63265.84</td>\n",
       "      <td>170032.88 ± 41689.96</td>\n",
       "      <td>0.8638 ± 0.0669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVR</th>\n",
       "      <td>294315.76 ± 66278.38</td>\n",
       "      <td>203735.30 ± 37283.13</td>\n",
       "      <td>0.8163 ± 0.1091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     RMSE                   MAE  \\\n",
       "Random Forest        156257.64 ± 49933.23   97521.36 ± 20437.54   \n",
       "LightGBM             160996.91 ± 47955.83  104886.76 ± 22178.11   \n",
       "Gradient Boosting    161645.37 ± 51150.56  105308.97 ± 24420.13   \n",
       "XGBoost              176521.35 ± 43506.95  117056.09 ± 31471.16   \n",
       "Ridge Regression     219276.49 ± 48004.33  164327.65 ± 28665.78   \n",
       "Linear Regression    219938.01 ± 47881.02  164794.68 ± 28645.08   \n",
       "ElasticNet           233678.26 ± 37408.87  180186.17 ± 22737.91   \n",
       "Lasso Regression     235249.19 ± 53711.19  180281.78 ± 45820.19   \n",
       "Decision Tree        227238.24 ± 95180.41  139010.16 ± 53233.58   \n",
       "AdaBoost             238869.93 ± 86784.19  192173.84 ± 78219.22   \n",
       "K-Nearest Neighbors  259447.02 ± 63265.84  170032.88 ± 41689.96   \n",
       "SVR                  294315.76 ± 66278.38  203735.30 ± 37283.13   \n",
       "\n",
       "                                  R²  \n",
       "Random Forest        0.9428 ± 0.0481  \n",
       "LightGBM             0.9422 ± 0.0450  \n",
       "Gradient Boosting    0.9417 ± 0.0469  \n",
       "XGBoost              0.9342 ± 0.0426  \n",
       "Ridge Regression     0.8965 ± 0.0654  \n",
       "Linear Regression    0.8961 ± 0.0652  \n",
       "ElasticNet           0.8870 ± 0.0577  \n",
       "Lasso Regression     0.8851 ± 0.0631  \n",
       "Decision Tree        0.8715 ± 0.1326  \n",
       "AdaBoost             0.8683 ± 0.0875  \n",
       "K-Nearest Neighbors  0.8638 ± 0.0669  \n",
       "SVR                  0.8163 ± 0.1091  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df = results_df.sort_values('R2_mean', ascending=False)\n",
    "\n",
    "# Format for display\n",
    "results_display = results_df.copy()\n",
    "results_display['RMSE'] = results_display.apply(lambda x: f\"{x['RMSE_mean']:.2f} ± {x['RMSE_std']:.2f}\", axis=1)\n",
    "results_display['MAE'] = results_display.apply(lambda x: f\"{x['MAE_mean']:.2f} ± {x['MAE_std']:.2f}\", axis=1)\n",
    "results_display['R²'] = results_display.apply(lambda x: f\"{x['R2_mean']:.4f} ± {x['R2_std']:.4f}\", axis=1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON RESULTS (sorted by R² score)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nMetrics averaged over 10-fold Time Series Cross-Validation:\")\n",
    "print()\n",
    "results_display[['RMSE', 'MAE', 'R²']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a1ad956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Best Model: Random Forest\n",
      "   - R² Score: 0.9428\n",
      "   - RMSE: 156257.64\n",
      "   - MAE: 97521.36\n",
      "\n",
      "Target variable (slp) statistics:\n",
      "   - Mean: 988192.36\n",
      "   - Std: 752107.47\n",
      "   - Min: -435171.56\n",
      "   - Max: 3341410.76\n"
     ]
    }
   ],
   "source": [
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "best_model = results_df['R2_mean'].idxmax()\n",
    "best_r2 = results_df.loc[best_model, 'R2_mean']\n",
    "best_rmse = results_df.loc[best_model, 'RMSE_mean']\n",
    "best_mae = results_df.loc[best_model, 'MAE_mean']\n",
    "\n",
    "print(f\"\\nBest Model: {best_model}\")\n",
    "print(f\"   - R² Score: {best_r2:.4f}\")\n",
    "print(f\"   - RMSE: {best_rmse:.2f}\")\n",
    "print(f\"   - MAE: {best_mae:.2f}\")\n",
    "\n",
    "print(f\"\\nTarget variable (slp) statistics:\")\n",
    "print(f\"   - Mean: {y.mean():.2f}\")\n",
    "print(f\"   - Std: {y.std():.2f}\")\n",
    "print(f\"   - Min: {y.min():.2f}\")\n",
    "print(f\"   - Max: {y.max():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c02fc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "R² SCORE COMPARISON\n",
      "================================================================================\n",
      "\n",
      "Random Forest             | ██████████████████████████████████████████████████ 0.9428\n",
      "LightGBM                  | █████████████████████████████████████████████████ 0.9422\n",
      "Gradient Boosting         | █████████████████████████████████████████████████ 0.9417\n",
      "XGBoost                   | █████████████████████████████████████████████████ 0.9342\n",
      "Ridge Regression          | ███████████████████████████████████████████████ 0.8965\n",
      "Linear Regression         | ███████████████████████████████████████████████ 0.8961\n",
      "ElasticNet                | ███████████████████████████████████████████████ 0.8870\n",
      "Lasso Regression          | ██████████████████████████████████████████████ 0.8851\n",
      "Decision Tree             | ██████████████████████████████████████████████ 0.8715\n",
      "AdaBoost                  | ██████████████████████████████████████████████ 0.8683\n",
      "K-Nearest Neighbors       | █████████████████████████████████████████████ 0.8638\n",
      "SVR                       | ███████████████████████████████████████████ 0.8163\n"
     ]
    }
   ],
   "source": [
    "# Visual comparison (text-based bar chart)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"R² SCORE COMPARISON\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "max_bar_length = 50\n",
    "max_r2 = results_df['R2_mean'].max()\n",
    "\n",
    "for model_name in results_df.index:\n",
    "    r2 = results_df.loc[model_name, 'R2_mean']\n",
    "    if r2 > 0:\n",
    "        bar_length = int((r2 / max_r2) * max_bar_length)\n",
    "        bar = '█' * bar_length\n",
    "    else:\n",
    "        bar_length = 0\n",
    "        bar = ''\n",
    "    print(f\"{model_name:25s} | {bar} {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7031b789",
   "metadata": {},
   "source": [
    "## 7. Optimal Training Timespan Analysis\n",
    "\n",
    "This section determines the optimal amount of historical data for predicting one year ahead.\n",
    "We use the last year of data as the test set and vary the training period from 1 year to all available historical data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af45d872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test period: 2024-09-30 to 2025-09-30\n",
      "Test set size: 364 samples\n",
      "\n",
      "Available training period: 2016-01-01 to 2024-09-30\n",
      "Total available training data: 8.7 years (3196 samples)\n"
     ]
    }
   ],
   "source": [
    "# Define the test period: last 1 year of data\n",
    "test_end_date = df_clean['date'].max()\n",
    "test_start_date = test_end_date - pd.DateOffset(years=1)\n",
    "\n",
    "# Create test set mask\n",
    "test_mask = df_clean['date'] > test_start_date\n",
    "X_test_final = X_scaled[test_mask]\n",
    "y_test_final = y[test_mask]\n",
    "\n",
    "print(f\"Test period: {test_start_date.date()} to {test_end_date.date()}\")\n",
    "print(f\"Test set size: {len(X_test_final)} samples\")\n",
    "\n",
    "# Available training data (everything before test period)\n",
    "train_available_mask = df_clean['date'] <= test_start_date\n",
    "train_start_date = df_clean[train_available_mask]['date'].min()\n",
    "train_end_date = df_clean[train_available_mask]['date'].max()\n",
    "\n",
    "print(f\"\\nAvailable training period: {train_start_date.date()} to {train_end_date.date()}\")\n",
    "total_train_years = (train_end_date - train_start_date).days / 365.25\n",
    "print(f\"Total available training data: {total_train_years:.1f} years ({train_available_mask.sum()} samples)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4cccbbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing training periods from 1 to 8 years\n",
      "\n",
      "Evaluating models with different training timespans...\n",
      "======================================================================\n",
      "\n",
      "1 year(s) of training data: 2023-09-30 to 2024-09-30 (366 samples)\n",
      "  Gradient Boosting: R² = 0.9536, RMSE = 148428.81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LightGBM: R² = 0.9539, RMSE = 147998.12\n",
      "  Random Forest: R² = 0.9474, RMSE = 158054.26\n",
      "\n",
      "2 year(s) of training data: 2022-09-30 to 2024-09-30 (731 samples)\n",
      "  Gradient Boosting: R² = 0.9487, RMSE = 156075.34\n",
      "  LightGBM: R² = 0.9581, RMSE = 141054.17\n",
      "  Random Forest: R² = 0.9441, RMSE = 162961.05\n",
      "\n",
      "3 year(s) of training data: 2021-09-30 to 2024-09-30 (1096 samples)\n",
      "  Gradient Boosting: R² = 0.9397, RMSE = 169137.47\n",
      "  LightGBM: R² = 0.9493, RMSE = 155164.54\n",
      "  Random Forest: R² = 0.9391, RMSE = 169992.50\n",
      "\n",
      "4 year(s) of training data: 2020-09-30 to 2024-09-30 (1461 samples)\n",
      "  Gradient Boosting: R² = 0.9007, RMSE = 217081.97\n",
      "  LightGBM: R² = 0.9586, RMSE = 140169.31\n",
      "  Random Forest: R² = 0.9336, RMSE = 177465.15\n",
      "\n",
      "5 year(s) of training data: 2019-09-30 to 2024-09-30 (1827 samples)\n",
      "  Gradient Boosting: R² = 0.9052, RMSE = 212148.12\n",
      "  LightGBM: R² = 0.9514, RMSE = 151885.24\n",
      "  Random Forest: R² = 0.9222, RMSE = 192117.55\n",
      "\n",
      "6 year(s) of training data: 2018-09-30 to 2024-09-30 (2192 samples)\n",
      "  Gradient Boosting: R² = 0.8703, RMSE = 248072.23\n",
      "  LightGBM: R² = 0.9517, RMSE = 151382.76\n",
      "  Random Forest: R² = 0.9183, RMSE = 196940.24\n",
      "\n",
      "7 year(s) of training data: 2017-09-30 to 2024-09-30 (2557 samples)\n",
      "  Gradient Boosting: R² = 0.9098, RMSE = 206944.18\n",
      "  LightGBM: R² = 0.9595, RMSE = 138697.84\n",
      "  Random Forest: R² = 0.9238, RMSE = 190193.93\n",
      "\n",
      "8 year(s) of training data: 2016-09-30 to 2024-09-30 (2922 samples)\n",
      "  Gradient Boosting: R² = 0.9269, RMSE = 186282.08\n",
      "  LightGBM: R² = 0.9562, RMSE = 144145.06\n",
      "  Random Forest: R² = 0.9212, RMSE = 193435.52\n",
      "\n",
      "======================================================================\n",
      "Training timespan analysis complete!\n"
     ]
    }
   ],
   "source": [
    "# Define the top 3 models based on previous results\n",
    "top_models = {\n",
    "    'Gradient Boosting': GradientBoostingRegressor(random_state=42),\n",
    "    'LightGBM': LGBMRegressor(random_state=42, n_jobs=-1, verbose=-1),\n",
    "    'Random Forest': RandomForestRegressor(random_state=42),\n",
    "}\n",
    "\n",
    "# Calculate max years available for training\n",
    "max_years = int(total_train_years)\n",
    "print(f\"Testing training periods from 1 to {max_years} years\\n\")\n",
    "\n",
    "# Store results for each training period\n",
    "timespan_results = {model_name: {'years': [], 'rmse': [], 'mae': [], 'r2': []} \n",
    "                    for model_name in top_models.keys()}\n",
    "\n",
    "print(\"Evaluating models with different training timespans...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for n_years in range(1, max_years + 1):\n",
    "    # Define training period: n_years before test_start_date\n",
    "    train_period_start = test_start_date - pd.DateOffset(years=n_years)\n",
    "    \n",
    "    # Create training mask for this period\n",
    "    train_mask = (df_clean['date'] > train_period_start) & (df_clean['date'] <= test_start_date)\n",
    "    \n",
    "    X_train = X_scaled[train_mask]\n",
    "    y_train = y[train_mask]\n",
    "    \n",
    "    print(f\"\\n{n_years} year(s) of training data: {train_period_start.date()} to {test_start_date.date()} ({len(X_train)} samples)\")\n",
    "    \n",
    "    for model_name, model in top_models.items():\n",
    "        # Clone the model to avoid refitting issues\n",
    "        from sklearn.base import clone\n",
    "        model_clone = clone(model)\n",
    "        \n",
    "        # Train and predict\n",
    "        model_clone.fit(X_train, y_train)\n",
    "        y_pred = model_clone.predict(X_test_final)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        rmse = np.sqrt(mean_squared_error(y_test_final, y_pred))\n",
    "        mae = mean_absolute_error(y_test_final, y_pred)\n",
    "        r2 = r2_score(y_test_final, y_pred)\n",
    "        \n",
    "        # Store results\n",
    "        timespan_results[model_name]['years'].append(n_years)\n",
    "        timespan_results[model_name]['rmse'].append(rmse)\n",
    "        timespan_results[model_name]['mae'].append(mae)\n",
    "        timespan_results[model_name]['r2'].append(r2)\n",
    "        \n",
    "        print(f\"  {model_name}: R² = {r2:.4f}, RMSE = {rmse:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Training timespan analysis complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f843b60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "R² SCORES BY TRAINING TIMESPAN\n",
      "================================================================================\n",
      "Model           Gradient Boosting  LightGBM  Random Forest\n",
      "Training Years                                            \n",
      "1                          0.9536    0.9539         0.9474\n",
      "2                          0.9487    0.9581         0.9441\n",
      "3                          0.9397    0.9493         0.9391\n",
      "4                          0.9007    0.9586         0.9336\n",
      "5                          0.9052    0.9514         0.9222\n",
      "6                          0.8703    0.9517         0.9183\n",
      "7                          0.9098    0.9595         0.9238\n",
      "8                          0.9269    0.9562         0.9212\n",
      "\n",
      "================================================================================\n",
      "RMSE BY TRAINING TIMESPAN\n",
      "================================================================================\n",
      "Model           Gradient Boosting   LightGBM  Random Forest\n",
      "Training Years                                             \n",
      "1                       148428.81  147998.12      158054.26\n",
      "2                       156075.34  141054.17      162961.05\n",
      "3                       169137.47  155164.54      169992.50\n",
      "4                       217081.97  140169.31      177465.15\n",
      "5                       212148.12  151885.24      192117.55\n",
      "6                       248072.23  151382.76      196940.24\n",
      "7                       206944.18  138697.84      190193.93\n",
      "8                       186282.08  144145.06      193435.52\n"
     ]
    }
   ],
   "source": [
    "# Create a comprehensive results DataFrame\n",
    "timespan_df_list = []\n",
    "for model_name, results in timespan_results.items():\n",
    "    for i in range(len(results['years'])):\n",
    "        timespan_df_list.append({\n",
    "            'Model': model_name,\n",
    "            'Training Years': results['years'][i],\n",
    "            'RMSE': results['rmse'][i],\n",
    "            'MAE': results['mae'][i],\n",
    "            'R²': results['r2'][i]\n",
    "        })\n",
    "\n",
    "timespan_df = pd.DataFrame(timespan_df_list)\n",
    "\n",
    "# Pivot table for R² scores\n",
    "r2_pivot = timespan_df.pivot(index='Training Years', columns='Model', values='R²')\n",
    "rmse_pivot = timespan_df.pivot(index='Training Years', columns='Model', values='RMSE')\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"R² SCORES BY TRAINING TIMESPAN\")\n",
    "print(\"=\" * 80)\n",
    "print(r2_pivot.round(4).to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RMSE BY TRAINING TIMESPAN\")\n",
    "print(\"=\" * 80)\n",
    "print(rmse_pivot.round(2).to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9aea379c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "OPTIMAL TRAINING TIMESPAN ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Gradient Boosting:\n",
      "  Optimal training period: 1 year(s)\n",
      "  Best R²: 0.9536\n",
      "  RMSE: 148428.81\n",
      "  MAE: 104028.40\n",
      "\n",
      "LightGBM:\n",
      "  Optimal training period: 7 year(s)\n",
      "  Best R²: 0.9595\n",
      "  RMSE: 138697.84\n",
      "  MAE: 89270.41\n",
      "\n",
      "Random Forest:\n",
      "  Optimal training period: 1 year(s)\n",
      "  Best R²: 0.9474\n",
      "  RMSE: 158054.26\n",
      "  MAE: 107460.47\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "OVERALL RECOMMENDATION (averaged across all 3 models)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Optimal training timespan: 1 year(s)\n",
      "Average R² score: 0.9516\n"
     ]
    }
   ],
   "source": [
    "# Find optimal training timespan for each model\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"OPTIMAL TRAINING TIMESPAN ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "optimal_timespans = {}\n",
    "for model_name in top_models.keys():\n",
    "    model_data = timespan_df[timespan_df['Model'] == model_name]\n",
    "    best_idx = model_data['R²'].idxmax()\n",
    "    best_row = timespan_df.loc[best_idx]\n",
    "    optimal_timespans[model_name] = {\n",
    "        'years': int(best_row['Training Years']),\n",
    "        'r2': best_row['R²'],\n",
    "        'rmse': best_row['RMSE'],\n",
    "        'mae': best_row['MAE']\n",
    "    }\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Optimal training period: {optimal_timespans[model_name]['years']} year(s)\")\n",
    "    print(f\"  Best R²: {optimal_timespans[model_name]['r2']:.4f}\")\n",
    "    print(f\"  RMSE: {optimal_timespans[model_name]['rmse']:.2f}\")\n",
    "    print(f\"  MAE: {optimal_timespans[model_name]['mae']:.2f}\")\n",
    "\n",
    "# Calculate average R² for each training timespan across all models\n",
    "avg_r2_by_years = timespan_df.groupby('Training Years')['R²'].mean()\n",
    "optimal_years_overall = avg_r2_by_years.idxmax()\n",
    "optimal_r2_overall = avg_r2_by_years.max()\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"OVERALL RECOMMENDATION (averaged across all 3 models)\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"\\nOptimal training timespan: {optimal_years_overall} year(s)\")\n",
    "print(f\"Average R² score: {optimal_r2_overall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a145add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "R² SCORE BY TRAINING TIMESPAN (Visual)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Gradient Boosting:\n",
      "   1 year(s) | ████████████████████████████████████████ 0.9536 ← BEST\n",
      "   2 year(s) | ███████████████████████████████████████ 0.9487\n",
      "   3 year(s) | ███████████████████████████████████████ 0.9397\n",
      "   4 year(s) | █████████████████████████████████████ 0.9007\n",
      "   5 year(s) | █████████████████████████████████████ 0.9052\n",
      "   6 year(s) | ████████████████████████████████████ 0.8703\n",
      "   7 year(s) | ██████████████████████████████████████ 0.9098\n",
      "   8 year(s) | ██████████████████████████████████████ 0.9269\n",
      "\n",
      "LightGBM:\n",
      "   1 year(s) | ███████████████████████████████████████ 0.9539\n",
      "   2 year(s) | ███████████████████████████████████████ 0.9581\n",
      "   3 year(s) | ███████████████████████████████████████ 0.9493\n",
      "   4 year(s) | ███████████████████████████████████████ 0.9586\n",
      "   5 year(s) | ███████████████████████████████████████ 0.9514\n",
      "   6 year(s) | ███████████████████████████████████████ 0.9517\n",
      "   7 year(s) | ████████████████████████████████████████ 0.9595 ← BEST\n",
      "   8 year(s) | ███████████████████████████████████████ 0.9562\n",
      "\n",
      "Random Forest:\n",
      "   1 year(s) | ████████████████████████████████████████ 0.9474 ← BEST\n",
      "   2 year(s) | ███████████████████████████████████████ 0.9441\n",
      "   3 year(s) | ███████████████████████████████████████ 0.9391\n",
      "   4 year(s) | ███████████████████████████████████████ 0.9336\n",
      "   5 year(s) | ██████████████████████████████████████ 0.9222\n",
      "   6 year(s) | ██████████████████████████████████████ 0.9183\n",
      "   7 year(s) | ███████████████████████████████████████ 0.9238\n",
      "   8 year(s) | ██████████████████████████████████████ 0.9212\n",
      "\n",
      "Average (all models):\n",
      "   1 year(s) | ████████████████████████████████████████ 0.9516 ← BEST\n",
      "   2 year(s) | ███████████████████████████████████████ 0.9503\n",
      "   3 year(s) | ███████████████████████████████████████ 0.9427\n",
      "   4 year(s) | ███████████████████████████████████████ 0.9310\n",
      "   5 year(s) | ██████████████████████████████████████ 0.9263\n",
      "   6 year(s) | ██████████████████████████████████████ 0.9134\n",
      "   7 year(s) | ███████████████████████████████████████ 0.9310\n",
      "   8 year(s) | ███████████████████████████████████████ 0.9348\n"
     ]
    }
   ],
   "source": [
    "# Visual comparison of R² scores by training timespan\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"R² SCORE BY TRAINING TIMESPAN (Visual)\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "for model_name in top_models.keys():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    model_data = timespan_df[timespan_df['Model'] == model_name].sort_values('Training Years')\n",
    "    \n",
    "    max_r2 = model_data['R²'].max()\n",
    "    for _, row in model_data.iterrows():\n",
    "        years = int(row['Training Years'])\n",
    "        r2 = row['R²']\n",
    "        bar_length = int((r2 / max_r2) * 40) if r2 > 0 else 0\n",
    "        bar = '█' * bar_length\n",
    "        marker = ' ← BEST' if r2 == max_r2 else ''\n",
    "        print(f\"  {years:2d} year(s) | {bar} {r2:.4f}{marker}\")\n",
    "\n",
    "# Average across models\n",
    "print(f\"\\nAverage (all models):\")\n",
    "for years in sorted(avg_r2_by_years.index):\n",
    "    r2 = avg_r2_by_years[years]\n",
    "    bar_length = int((r2 / avg_r2_by_years.max()) * 40) if r2 > 0 else 0\n",
    "    bar = '█' * bar_length\n",
    "    marker = ' ← BEST' if years == optimal_years_overall else ''\n",
    "    print(f\"  {years:2d} year(s) | {bar} {r2:.4f}{marker}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08432e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FINAL SUMMARY: OPTIMAL TRAINING TIMESPAN\n",
      "================================================================================\n",
      "\n",
      "Test Period: 2024-09-30 to 2025-09-30 (364 days)\n",
      "\n",
      "Results by Model:\n",
      "\n",
      "  • Gradient Boosting: 1 year(s) → R² = 0.9536\n",
      "  • LightGBM: 7 year(s) → R² = 0.9595\n",
      "  • Random Forest: 1 year(s) → R² = 0.9474\n",
      "\n",
      "════════════════════════════════════════════════════════════════════════════════\n",
      "RECOMMENDATION: Use 1 year(s) of historical data for training\n",
      "                when predicting 1 year ahead.\n",
      "                \n",
      "                This achieves an average R² of 0.9516 across the\n",
      "                top 3 models (Random Forest, Gradient Boosting, LightGBM).\n",
      "════════════════════════════════════════════════════════════════════════════════\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL SUMMARY: OPTIMAL TRAINING TIMESPAN\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\"\"\n",
    "Test Period: {test_start_date.date()} to {test_end_date.date()} ({len(y_test_final)} days)\n",
    "\n",
    "Results by Model:\n",
    "\"\"\")\n",
    "\n",
    "for model_name, opt in optimal_timespans.items():\n",
    "    print(f\"  • {model_name}: {opt['years']} year(s) → R² = {opt['r2']:.4f}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "════════════════════════════════════════════════════════════════════════════════\n",
    "RECOMMENDATION: Use {optimal_years_overall} year(s) of historical data for training\n",
    "                when predicting 1 year ahead.\n",
    "                \n",
    "                This achieves an average R² of {optimal_r2_overall:.4f} across the\n",
    "                top 3 models (Random Forest, Gradient Boosting, LightGBM).\n",
    "════════════════════════════════════════════════════════════════════════════════\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d8214d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported data to 'dataset/data_v3_rf.csv'\n",
      "\n",
      "Data period: 2023-09-30 to 2025-09-30\n",
      "  - Training period: 2023-09-30 to 2024-09-30 (1 years)\n",
      "  - Test period: 2024-09-30 to 2025-09-30 (1 year)\n",
      "\n",
      "Total samples: 730\n",
      "Shape: (730, 9)\n"
     ]
    }
   ],
   "source": [
    "# Export data with optimal training timespan + test timespan\n",
    "# Define the optimal training period start date\n",
    "optimal_train_start = test_start_date - pd.DateOffset(years=optimal_years_overall)\n",
    "\n",
    "# Create mask for optimal training + test data\n",
    "export_mask = df_clean['date'] > optimal_train_start\n",
    "\n",
    "# Create export dataframe with scaled features\n",
    "export_df_optimal = X_scaled[export_mask].copy()\n",
    "export_df_optimal['slp'] = y[export_mask].values\n",
    "export_df_optimal['date'] = df_clean.loc[export_mask, 'date'].values\n",
    "\n",
    "# Reorder columns to put date first\n",
    "cols = ['date', 'slp'] + [col for col in export_df_optimal.columns if col not in ['date', 'slp']]\n",
    "export_df_optimal = export_df_optimal[cols]\n",
    "\n",
    "# Save to CSV\n",
    "export_path = 'dataset/data_v3_rf.csv'\n",
    "export_df_optimal.to_csv(export_path, sep=';', decimal=',', index=False)\n",
    "\n",
    "print(f\"Exported data to '{export_path}'\")\n",
    "print(f\"\\nData period: {optimal_train_start.date()} to {test_end_date.date()}\")\n",
    "print(f\"  - Training period: {optimal_train_start.date()} to {test_start_date.date()} ({optimal_years_overall} years)\")\n",
    "print(f\"  - Test period: {test_start_date.date()} to {test_end_date.date()} (1 year)\")\n",
    "print(f\"\\nTotal samples: {len(export_df_optimal)}\")\n",
    "print(f\"Shape: {export_df_optimal.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
