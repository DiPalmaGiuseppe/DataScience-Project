{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b0eed87",
   "metadata": {},
   "source": [
    "# Model Selection for SLP Prediction (Multi-Dataset)\n",
    "\n",
    "This notebook performs model selection to predict the `slp` column using various machine learning algorithms with time series cross-validation.\n",
    "\n",
    "**Processes 3 datasets:**\n",
    "- `df_clean_stat.csv` → `data_v3_stat.csv` (statistical features)\n",
    "- `df_clean_rf.csv` → `data_v3_rf.csv` (random forest features)\n",
    "- `data_v2_full.csv` → `data_v3_full.csv` (all features with cyclical encoding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0b7581f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured 3 datasets to process:\n",
      "  - stat: dataset/df_clean_stat.csv → dataset/data_v3_stat.csv\n",
      "  - rf: dataset/df_clean_rf.csv → dataset/data_v3_rf.csv\n",
      "  - full: dataset/data_v2_full.csv → dataset/data_v3_full.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.base import clone\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define all datasets to process\n",
    "DATASETS = [\n",
    "    {\n",
    "        'name': 'stat',\n",
    "        'input_file': 'dataset/df_clean_stat.csv',\n",
    "        'output_file': 'dataset/data_v3_stat.csv',\n",
    "        'target_col': 'Residential (SLP)',\n",
    "        'has_cyclical_cols': False,\n",
    "        'drop_cols': ['rlm', 'entry'],\n",
    "    },\n",
    "    {\n",
    "        'name': 'rf',\n",
    "        'input_file': 'dataset/df_clean_rf.csv',\n",
    "        'output_file': 'dataset/data_v3_rf.csv',\n",
    "        'target_col': 'slp',\n",
    "        'has_cyclical_cols': False,\n",
    "        'drop_cols': ['idx'],\n",
    "    },\n",
    "    {\n",
    "        'name': 'full',\n",
    "        'input_file': 'dataset/data_v2_full.csv',\n",
    "        'output_file': 'dataset/data_v3_full.csv',\n",
    "        'target_col': 'slp',\n",
    "        'has_cyclical_cols': True,\n",
    "        'drop_cols': ['rlm', 'entry'],\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Configured {len(DATASETS)} datasets to process:\")\n",
    "for ds in DATASETS:\n",
    "    print(f\"  - {ds['name']}: {ds['input_file']} → {ds['output_file']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05222b9e",
   "metadata": {},
   "source": [
    "## 1. Process All Datasets\n",
    "\n",
    "This cell loops through all 3 datasets, performing:\n",
    "1. Data loading and cleaning\n",
    "2. Feature preprocessing (cyclical encoding, scaling)\n",
    "3. Model training and evaluation with TimeSeriesSplit\n",
    "4. Optimal training timespan analysis\n",
    "5. Export of processed data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "253b1834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PROCESSING DATASET: STAT\n",
      "Input: dataset/df_clean_stat.csv\n",
      "================================================================================\n",
      "\n",
      "Dataset shape: (3560, 13)\n",
      "Columns: ['date', 'temperature_2m_mean', 'sunrise', 'et0_fao_evapotranspiration', 'sunshine_duration', 'snowfall_sum', 'day_of_year', 'precipitation_hours', 'weathercode', 'windspeed_10m_max', 'rain_sum', 'holiday', 'Residential (SLP)']\n",
      "No columns to remove from ['rlm', 'entry']\n",
      "\n",
      "Features shape: (3560, 11)\n",
      "Target shape: (3560,)\n",
      "Shape after encoding: (3560, 11)\n",
      "Scaled 9 continuous features\n",
      "\n",
      "Training and evaluating models...\n",
      "  Linear Regression: R² = 0.9116\n",
      "  Ridge Regression: R² = 0.9115\n",
      "  Lasso Regression: R² = 0.9116\n",
      "  ElasticNet: R² = 0.8508\n",
      "  Decision Tree: R² = 0.9076\n",
      "  Random Forest: R² = 0.9405\n",
      "  Gradient Boosting: R² = 0.9421\n",
      "  AdaBoost: R² = 0.9061\n",
      "  XGBoost: R² = 0.9377\n",
      "  LightGBM: R² = 0.9397\n",
      "  K-Nearest Neighbors: R² = 0.8858\n",
      "  SVR: R² = 0.8551\n",
      "\n",
      "Best Model: Gradient Boosting (R² = 0.9421)\n",
      "\n",
      "--- Training Timespan Analysis ---\n",
      "Test period: 2024-09-30 to 2025-09-30 (364 samples)\n",
      "Max training years available: 8\n",
      "\n",
      "Optimal training timespan: 2 year(s) (avg R² = 0.9753)\n",
      "\n",
      "Exported to 'dataset/data_v3_stat.csv' (1095 samples)\n",
      "\n",
      "================================================================================\n",
      "PROCESSING DATASET: RF\n",
      "Input: dataset/df_clean_rf.csv\n",
      "================================================================================\n",
      "\n",
      "Dataset shape: (3560, 9)\n",
      "Columns: ['idx', 'date', 'day_of_year', 'slp', 'apparent_temperature_mean', 'temperature_2m_mean', 'apparent_temperature_max', 'temperature_2m_max', 'sunrise']\n",
      "Removed columns: ['idx']\n",
      "\n",
      "Features shape: (3560, 6)\n",
      "Target shape: (3560,)\n",
      "Shape after encoding: (3560, 6)\n",
      "Scaled 6 continuous features\n",
      "\n",
      "Training and evaluating models...\n",
      "  Linear Regression: R² = 0.8920\n",
      "  Ridge Regression: R² = 0.8923\n",
      "  Lasso Regression: R² = 0.8921\n",
      "  ElasticNet: R² = 0.8850\n",
      "  Decision Tree: R² = 0.9137\n",
      "  Random Forest: R² = 0.9420\n",
      "  Gradient Boosting: R² = 0.9431\n",
      "  AdaBoost: R² = 0.8828\n",
      "  XGBoost: R² = 0.9371\n",
      "  LightGBM: R² = 0.9398\n",
      "  K-Nearest Neighbors: R² = 0.9394\n",
      "  SVR: R² = 0.9447\n",
      "\n",
      "Best Model: SVR (R² = 0.9447)\n",
      "\n",
      "--- Training Timespan Analysis ---\n",
      "Test period: 2024-09-30 to 2025-09-30 (364 samples)\n",
      "Max training years available: 8\n",
      "\n",
      "Optimal training timespan: 2 year(s) (avg R² = 0.9753)\n",
      "\n",
      "Exported to 'dataset/data_v3_rf.csv' (1095 samples)\n",
      "\n",
      "================================================================================\n",
      "PROCESSING DATASET: FULL\n",
      "Input: dataset/data_v2_full.csv\n",
      "================================================================================\n",
      "\n",
      "Dataset shape: (3560, 26)\n",
      "Columns: ['date', 'entry', 'rlm', 'slp', 'day_of_year', 'day_of_week', 'holiday', 'weathercode', 'temperature_2m_max', 'temperature_2m_min', 'temperature_2m_mean', 'apparent_temperature_max', 'apparent_temperature_min', 'apparent_temperature_mean', 'sunrise', 'sunset', 'daylight_duration', 'sunshine_duration', 'rain_sum', 'snowfall_sum', 'precipitation_hours', 'windspeed_10m_max', 'windgusts_10m_max', 'winddirection_10m_dominant', 'shortwave_radiation_sum', 'et0_fao_evapotranspiration']\n",
      "Removed columns: ['rlm', 'entry']\n",
      "\n",
      "Features shape: (3560, 22)\n",
      "Target shape: (3560,)\n",
      "Applied cyclical encoding, dropped: ['day_of_week', 'day_of_year', 'winddirection_10m_dominant']\n",
      "Shape after encoding: (3560, 25)\n",
      "Scaled 23 continuous features\n",
      "\n",
      "Training and evaluating models...\n",
      "  Linear Regression: R² = 0.9351\n",
      "  Ridge Regression: R² = 0.9333\n",
      "  Lasso Regression: R² = 0.9314\n",
      "  ElasticNet: R² = 0.8940\n",
      "  Decision Tree: R² = 0.9118\n",
      "  Random Forest: R² = 0.9459\n",
      "  Gradient Boosting: R² = 0.9454\n",
      "  AdaBoost: R² = 0.9134\n",
      "  XGBoost: R² = 0.9391\n",
      "  LightGBM: R² = 0.9430\n",
      "  K-Nearest Neighbors: R² = 0.9057\n",
      "  SVR: R² = 0.9110\n",
      "\n",
      "Best Model: Random Forest (R² = 0.9459)\n",
      "\n",
      "--- Training Timespan Analysis ---\n",
      "Test period: 2024-09-30 to 2025-09-30 (364 samples)\n",
      "Max training years available: 8\n",
      "\n",
      "Optimal training timespan: 3 year(s) (avg R² = 0.9748)\n",
      "\n",
      "Exported to 'dataset/data_v3_full.csv' (1460 samples)\n",
      "\n",
      "================================================================================\n",
      "ALL DATASETS PROCESSED SUCCESSFULLY!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Store results for all datasets\n",
    "all_results = {}\n",
    "all_timespan_results = {}\n",
    "\n",
    "# Process each dataset\n",
    "for dataset_config in DATASETS:\n",
    "    dataset_name = dataset_config['name']\n",
    "    dataset_file = dataset_config['input_file']\n",
    "    output_file = dataset_config['output_file']\n",
    "    target_col = dataset_config['target_col']\n",
    "    has_cyclical = dataset_config['has_cyclical_cols']\n",
    "    drop_cols = dataset_config['drop_cols']\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"PROCESSING DATASET: {dataset_name.upper()}\")\n",
    "    print(f\"Input: {dataset_file}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(dataset_file, sep=';', decimal=',')\n",
    "    print(f\"\\nDataset shape: {df.shape}\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    # Remove specified columns if present\n",
    "    to_remove = [col for col in drop_cols if col in df.columns]\n",
    "    if to_remove:\n",
    "        df_clean = df.drop(columns=to_remove)\n",
    "        print(f\"Removed columns: {to_remove}\")\n",
    "    else:\n",
    "        df_clean = df.copy()\n",
    "        print(f\"No columns to remove from {drop_cols}\")\n",
    "    \n",
    "    # Parse date and sort\n",
    "    df_clean['date'] = pd.to_datetime(df_clean['date'])\n",
    "    df_clean = df_clean.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = df_clean.drop(columns=['date', target_col])\n",
    "    y = df_clean[target_col]\n",
    "    \n",
    "    # Define feature types for proper preprocessing\n",
    "    boolean_cols = ['holiday'] if 'holiday' in X.columns else []\n",
    "    categorical_cols = ['weathercode'] if 'weathercode' in X.columns else []\n",
    "    cyclical_cols = ['day_of_week', 'day_of_year', 'winddirection_10m_dominant'] if has_cyclical else []\n",
    "    \n",
    "    # All other columns are continuous and should be scaled\n",
    "    continuous_cols = [col for col in X.columns \n",
    "                       if col not in boolean_cols + categorical_cols + cyclical_cols]\n",
    "    \n",
    "    # Convert continuous columns to numeric\n",
    "    for col in continuous_cols:\n",
    "        X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "    y = pd.to_numeric(y, errors='coerce')\n",
    "    \n",
    "    print(f\"\\nFeatures shape: {X.shape}\")\n",
    "    print(f\"Target shape: {y.shape}\")\n",
    "    \n",
    "    # Apply cyclical encoding if needed\n",
    "    def cyclical_encode(df, col, max_val):\n",
    "        df[f'{col}_sin'] = np.sin(2 * np.pi * df[col] / max_val)\n",
    "        df[f'{col}_cos'] = np.cos(2 * np.pi * df[col] / max_val)\n",
    "        return df\n",
    "    \n",
    "    X_encoded = X.copy()\n",
    "    \n",
    "    if has_cyclical:\n",
    "        if 'day_of_week' in X_encoded.columns:\n",
    "            X_encoded = cyclical_encode(X_encoded, 'day_of_week', 7)\n",
    "        if 'winddirection_10m_dominant' in X_encoded.columns:\n",
    "            X_encoded = cyclical_encode(X_encoded, 'winddirection_10m_dominant', 360)\n",
    "        if 'day_of_year' in X_encoded.columns:\n",
    "            X_encoded = cyclical_encode(X_encoded, 'day_of_year', 366)\n",
    "        # Drop original cyclical columns\n",
    "        cols_to_drop = [c for c in cyclical_cols if c in X_encoded.columns]\n",
    "        X_encoded = X_encoded.drop(columns=cols_to_drop)\n",
    "        print(f\"Applied cyclical encoding, dropped: {cols_to_drop}\")\n",
    "    \n",
    "    print(f\"Shape after encoding: {X_encoded.shape}\")\n",
    "    \n",
    "    # Update continuous_cols after cyclical encoding\n",
    "    continuous_cols = [col for col in X_encoded.columns \n",
    "                       if col not in boolean_cols + categorical_cols]\n",
    "    \n",
    "    # Apply StandardScaler to continuous features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = X_encoded.copy()\n",
    "    if continuous_cols:\n",
    "        X_scaled[continuous_cols] = scaler.fit_transform(X_encoded[continuous_cols])\n",
    "    \n",
    "    print(f\"Scaled {len(continuous_cols)} continuous features\")\n",
    "    \n",
    "    # Time Series Split\n",
    "    tscv = TimeSeriesSplit(n_splits=10)\n",
    "    \n",
    "    # Define models\n",
    "    models = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Ridge Regression': Ridge(),\n",
    "        'Lasso Regression': Lasso(),\n",
    "        'ElasticNet': ElasticNet(),\n",
    "        'Decision Tree': DecisionTreeRegressor(random_state=42),\n",
    "        'Random Forest': RandomForestRegressor(random_state=42),\n",
    "        'Gradient Boosting': GradientBoostingRegressor(random_state=42),\n",
    "        'AdaBoost': AdaBoostRegressor(random_state=42),\n",
    "        'XGBoost': XGBRegressor(random_state=42, n_jobs=-1, verbosity=0),\n",
    "        'LightGBM': LGBMRegressor(random_state=42, n_jobs=-1, verbose=-1),\n",
    "        'K-Nearest Neighbors': KNeighborsRegressor(),\n",
    "        'SVR': SVR(C=1e6, epsilon=1e4, kernel='rbf'),\n",
    "    }\n",
    "    \n",
    "    # Evaluate models function\n",
    "    def evaluate_model(model, X, y, tscv):\n",
    "        rmse_scores, mae_scores, r2_scores = [], [], []\n",
    "        for train_idx, test_idx in tscv.split(X):\n",
    "            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            rmse_scores.append(np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "            mae_scores.append(mean_absolute_error(y_test, y_pred))\n",
    "            r2_scores.append(r2_score(y_test, y_pred))\n",
    "        return {\n",
    "            'RMSE_mean': np.mean(rmse_scores), 'RMSE_std': np.std(rmse_scores),\n",
    "            'MAE_mean': np.mean(mae_scores), 'MAE_std': np.std(mae_scores),\n",
    "            'R2_mean': np.mean(r2_scores), 'R2_std': np.std(r2_scores),\n",
    "        }\n",
    "    \n",
    "    # Train and evaluate all models\n",
    "    print(\"\\nTraining and evaluating models...\")\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            metrics = evaluate_model(clone(model), X_scaled, y, tscv)\n",
    "            results[name] = metrics\n",
    "            print(f\"  {name}: R² = {metrics['R2_mean']:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  {name}: Error - {str(e)}\")\n",
    "            results[name] = {'RMSE_mean': np.nan, 'MAE_mean': np.nan, 'R2_mean': np.nan}\n",
    "    \n",
    "    all_results[dataset_name] = results\n",
    "    \n",
    "    # Find best model\n",
    "    results_df = pd.DataFrame(results).T\n",
    "    results_df = results_df.sort_values('R2_mean', ascending=False)\n",
    "    best_model = results_df['R2_mean'].idxmax()\n",
    "    print(f\"\\nBest Model: {best_model} (R² = {results_df.loc[best_model, 'R2_mean']:.4f})\")\n",
    "    \n",
    "    # Optimal Training Timespan Analysis\n",
    "    print(\"\\n--- Training Timespan Analysis ---\")\n",
    "    \n",
    "    test_end_date = df_clean['date'].max()\n",
    "    test_start_date = test_end_date - pd.DateOffset(years=1)\n",
    "    test_mask = df_clean['date'] > test_start_date\n",
    "    X_test_final = X_scaled[test_mask]\n",
    "    y_test_final = y[test_mask]\n",
    "    \n",
    "    train_available_mask = df_clean['date'] <= test_start_date\n",
    "    train_start_date = df_clean[train_available_mask]['date'].min()\n",
    "    train_end_date = df_clean[train_available_mask]['date'].max()\n",
    "    total_train_years = (train_end_date - train_start_date).days / 365.25\n",
    "    max_years = int(total_train_years)\n",
    "    \n",
    "    print(f\"Test period: {test_start_date.date()} to {test_end_date.date()} ({len(X_test_final)} samples)\")\n",
    "    print(f\"Max training years available: {max_years}\")\n",
    "    \n",
    "    # Top 3 models for timespan analysis\n",
    "    top_models = {\n",
    "        'Gradient Boosting': GradientBoostingRegressor(random_state=42),\n",
    "        'LightGBM': LGBMRegressor(random_state=42, n_jobs=-1, verbose=-1),\n",
    "        'Random Forest': RandomForestRegressor(random_state=42),\n",
    "    }\n",
    "    \n",
    "    timespan_results = {name: {'years': [], 'rmse': [], 'mae': [], 'r2': []} \n",
    "                        for name in top_models.keys()}\n",
    "    \n",
    "    for n_years in range(1, max_years + 1):\n",
    "        train_period_start = test_start_date - pd.DateOffset(years=n_years)\n",
    "        train_mask = (df_clean['date'] > train_period_start) & (df_clean['date'] <= test_start_date)\n",
    "        X_train = X_scaled[train_mask]\n",
    "        y_train = y[train_mask]\n",
    "        \n",
    "        for model_name, model in top_models.items():\n",
    "            model_clone = clone(model)\n",
    "            model_clone.fit(X_train, y_train)\n",
    "            y_pred = model_clone.predict(X_test_final)\n",
    "            rmse = np.sqrt(mean_squared_error(y_test_final, y_pred))\n",
    "            mae = mean_absolute_error(y_test_final, y_pred)\n",
    "            r2 = r2_score(y_test_final, y_pred)\n",
    "            timespan_results[model_name]['years'].append(n_years)\n",
    "            timespan_results[model_name]['rmse'].append(rmse)\n",
    "            timespan_results[model_name]['mae'].append(mae)\n",
    "            timespan_results[model_name]['r2'].append(r2)\n",
    "    \n",
    "    all_timespan_results[dataset_name] = timespan_results\n",
    "    \n",
    "    # Find optimal timespan\n",
    "    timespan_df_list = []\n",
    "    for model_name, res in timespan_results.items():\n",
    "        for i in range(len(res['years'])):\n",
    "            timespan_df_list.append({\n",
    "                'Model': model_name, 'Training Years': res['years'][i],\n",
    "                'RMSE': res['rmse'][i], 'MAE': res['mae'][i], 'R²': res['r2'][i]\n",
    "            })\n",
    "    timespan_df = pd.DataFrame(timespan_df_list)\n",
    "    \n",
    "    avg_r2_by_years = timespan_df.groupby('Training Years')['R²'].mean()\n",
    "    optimal_years_overall = avg_r2_by_years.idxmax()\n",
    "    optimal_r2_overall = avg_r2_by_years.max()\n",
    "    \n",
    "    print(f\"\\nOptimal training timespan: {optimal_years_overall} year(s) (avg R² = {optimal_r2_overall:.4f})\")\n",
    "    \n",
    "    # Export data with optimal training timespan\n",
    "    optimal_train_start = test_start_date - pd.DateOffset(years=optimal_years_overall)\n",
    "    export_mask = df_clean['date'] > optimal_train_start\n",
    "    \n",
    "    export_df_optimal = X_scaled[export_mask].copy()\n",
    "    export_df_optimal['slp'] = y[export_mask].values\n",
    "    export_df_optimal['date'] = df_clean.loc[export_mask, 'date'].values\n",
    "    \n",
    "    cols = ['date', 'slp'] + [col for col in export_df_optimal.columns if col not in ['date', 'slp']]\n",
    "    export_df_optimal = export_df_optimal[cols]\n",
    "    \n",
    "    export_df_optimal.to_csv(output_file, sep=';', decimal=',', index=False)\n",
    "    print(f\"\\nExported to '{output_file}' ({len(export_df_optimal)} samples)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ALL DATASETS PROCESSED SUCCESSFULLY!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2bbf40",
   "metadata": {},
   "source": [
    "## 2. Results Summary\n",
    "\n",
    "Display model performance and optimal training timespan for all datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6d190b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPREHENSIVE RESULTS SUMMARY - ALL DATASETS\n",
      "================================================================================\n",
      "\n",
      "────────────────────────────────────────\n",
      "Dataset: STAT\n",
      "────────────────────────────────────────\n",
      "\n",
      "Top 5 Models:\n",
      "                                   RMSE                   MAE               R²\n",
      "Gradient Boosting  152490.86 ± 53817.95   95596.49 ± 21410.88  0.9421 ± 0.0547\n",
      "Random Forest      154981.07 ± 53730.07   96681.97 ± 24726.36  0.9405 ± 0.0541\n",
      "LightGBM           157027.86 ± 52364.82   98323.66 ± 22699.18  0.9397 ± 0.0549\n",
      "XGBoost            160045.79 ± 52431.26   99812.10 ± 23490.64  0.9377 ± 0.0542\n",
      "Lasso Regression   197656.30 ± 48214.17  146446.35 ± 28582.40  0.9116 ± 0.0645\n",
      "\n",
      "────────────────────────────────────────\n",
      "Dataset: RF\n",
      "────────────────────────────────────────\n",
      "\n",
      "Top 5 Models:\n",
      "                                     RMSE                  MAE               R²\n",
      "SVR                  146579.26 ± 56414.68  91534.67 ± 26548.84  0.9447 ± 0.0551\n",
      "Gradient Boosting    151414.14 ± 52744.10  95270.30 ± 22528.45  0.9431 ± 0.0536\n",
      "Random Forest        153055.48 ± 53263.05  95556.41 ± 23342.47  0.9420 ± 0.0537\n",
      "LightGBM             157867.01 ± 50854.12  99109.83 ± 21444.03  0.9398 ± 0.0544\n",
      "K-Nearest Neighbors  156538.39 ± 54429.38  98034.16 ± 23663.14  0.9394 ± 0.0573\n",
      "\n",
      "────────────────────────────────────────\n",
      "Dataset: FULL\n",
      "────────────────────────────────────────\n",
      "\n",
      "Top 5 Models:\n",
      "                                   RMSE                   MAE               R²\n",
      "Random Forest      147236.68 ± 52006.04   91789.77 ± 22944.94  0.9459 ± 0.0510\n",
      "Gradient Boosting  146868.01 ± 54206.25   92336.57 ± 23086.48  0.9454 ± 0.0535\n",
      "LightGBM           151784.32 ± 52923.94   94378.77 ± 22908.23  0.9430 ± 0.0543\n",
      "XGBoost            156939.46 ± 53666.44   98155.29 ± 24121.72  0.9391 ± 0.0556\n",
      "Linear Regression  165298.66 ± 50131.84  117248.08 ± 25821.34  0.9351 ± 0.0557\n"
     ]
    }
   ],
   "source": [
    "# Summary of all results\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPREHENSIVE RESULTS SUMMARY - ALL DATASETS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for dataset_name, results in all_results.items():\n",
    "    print(f\"\\n{'─' * 40}\")\n",
    "    print(f\"Dataset: {dataset_name.upper()}\")\n",
    "    print(f\"{'─' * 40}\")\n",
    "    \n",
    "    results_df = pd.DataFrame(results).T\n",
    "    results_df = results_df.sort_values('R2_mean', ascending=False)\n",
    "    \n",
    "    # Format for display\n",
    "    results_display = results_df.copy()\n",
    "    results_display['RMSE'] = results_display.apply(lambda x: f\"{x['RMSE_mean']:.2f} ± {x['RMSE_std']:.2f}\", axis=1)\n",
    "    results_display['MAE'] = results_display.apply(lambda x: f\"{x['MAE_mean']:.2f} ± {x['MAE_std']:.2f}\", axis=1)\n",
    "    results_display['R²'] = results_display.apply(lambda x: f\"{x['R2_mean']:.4f} ± {x['R2_std']:.4f}\", axis=1)\n",
    "    \n",
    "    print(\"\\nTop 5 Models:\")\n",
    "    print(results_display[['RMSE', 'MAE', 'R²']].head().to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b918bbd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "OPTIMAL TRAINING TIMESPAN - ALL DATASETS\n",
      "================================================================================\n",
      "\n",
      "────────────────────────────────────────\n",
      "Dataset: STAT\n",
      "────────────────────────────────────────\n",
      "  Gradient Boosting: 2 year(s) → R² = 0.9747\n",
      "  LightGBM: 2 year(s) → R² = 0.9731\n",
      "  Random Forest: 2 year(s) → R² = 0.9781\n",
      "  >> Overall recommendation: 2 year(s)\n",
      "\n",
      "────────────────────────────────────────\n",
      "Dataset: RF\n",
      "────────────────────────────────────────\n",
      "  Gradient Boosting: 3 year(s) → R² = 0.9771\n",
      "  LightGBM: 2 year(s) → R² = 0.9747\n",
      "  Random Forest: 3 year(s) → R² = 0.9765\n",
      "  >> Overall recommendation: 2 year(s)\n",
      "\n",
      "────────────────────────────────────────\n",
      "Dataset: FULL\n",
      "────────────────────────────────────────\n",
      "  Gradient Boosting: 3 year(s) → R² = 0.9763\n",
      "  LightGBM: 2 year(s) → R² = 0.9749\n",
      "  Random Forest: 2 year(s) → R² = 0.9748\n",
      "  >> Overall recommendation: 3 year(s)\n",
      "\n",
      "================================================================================\n",
      "All output files have been saved to the dataset/ folder\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Timespan Analysis Summary\n",
    "print(\"=\" * 80)\n",
    "print(\"OPTIMAL TRAINING TIMESPAN - ALL DATASETS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for dataset_name, timespan_results in all_timespan_results.items():\n",
    "    print(f\"\\n{'─' * 40}\")\n",
    "    print(f\"Dataset: {dataset_name.upper()}\")\n",
    "    print(f\"{'─' * 40}\")\n",
    "    \n",
    "    # Build dataframe\n",
    "    timespan_df_list = []\n",
    "    for model_name, res in timespan_results.items():\n",
    "        for i in range(len(res['years'])):\n",
    "            timespan_df_list.append({\n",
    "                'Model': model_name, 'Years': res['years'][i],\n",
    "                'RMSE': res['rmse'][i], 'R²': res['r2'][i]\n",
    "            })\n",
    "    timespan_df = pd.DataFrame(timespan_df_list)\n",
    "    \n",
    "    # Find optimal for each model\n",
    "    for model_name in timespan_results.keys():\n",
    "        model_data = timespan_df[timespan_df['Model'] == model_name]\n",
    "        best_idx = model_data['R²'].idxmax()\n",
    "        best_row = timespan_df.loc[best_idx]\n",
    "        print(f\"  {model_name}: {int(best_row['Years'])} year(s) → R² = {best_row['R²']:.4f}\")\n",
    "    \n",
    "    # Overall recommendation\n",
    "    avg_r2 = timespan_df.groupby('Years')['R²'].mean()\n",
    "    optimal_years = avg_r2.idxmax()\n",
    "    print(f\"  >> Overall recommendation: {optimal_years} year(s)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"All output files have been saved to the dataset/ folder\")\n",
    "print(\"=\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
